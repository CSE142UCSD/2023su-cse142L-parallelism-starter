{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from notebook import *\n",
    "# if get something about NUMEXPR_MAX_THREADS being set incorrectly, don't worry.  It's not a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"namebox\">    \n",
    "Double Click to edit and enter your\n",
    "\n",
    "1.  Name\n",
    "2.  Student ID\n",
    "3.  @ucsd.edu email address\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "<div style=\" font-size: 300% !important;\n",
    "    margin-top: 1.5em;\n",
    "    margin-bottom: 10px;\n",
    "    font-weight: bold;\n",
    "    line-height: 1.0;\n",
    "    text-align:center;\">\n",
    "Lab 4: Exploiting the parallelism in modern computers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "In this lab, you'll learn about the concepts of:\n",
    "\n",
    "1.  Instruction-level parallelism\n",
    "2.  Memory level parallelsim\n",
    "3.  Superscalar/out-of-order pipeline\n",
    "4.  Branch prediction\n",
    "5.  Function calls and compiler optimizations\n",
    "\n",
    "You are strongly encouraged to go through the following documents before starting.\n",
    "1.  The x86-64 assembly http://www.cs.cmu.edu/~fp/courses/15213-s06/misc/asm64-handout.pdf\n",
    "2.  Intel Alder Lake CPU Architectures https://ieeexplore.ieee.org/document/9747991\n",
    "This lab includes a programming lab. \n",
    "\n",
    "Check the course schedule for due date(s).\n",
    "\n",
    "We need to thank [Dr. Steven Swanson](https://cseweb.ucsd.edu/~swanson/) as a significant part of the lab is orginated from Dr. Swanson's teaching materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# FAQ and Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "* There are no updates, yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Keeping Your Lab Up-to-Date\n",
    "\n",
    "Occasionally, there will be changes made to the base repository after the\n",
    "lab is released.  This may include bug fixes and updates to this document.  We'll post on piazza when an update is available.\n",
    "\n",
    "In those cases, you can use the following commands to pull the changes from upstream and merge them into your code.  You'll need to do this at a shell.  It won't work properly in the notebook.  **Save your notebook in the browser first**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "```\n",
    "cd <your directory for this lab>git remote add upstream $(cat .starter_repo)  # You need to do this once each time you checkout a new lab. It will fail \n",
    "                                              # harmlessly if you run it more than once.\n",
    "cp Lab.ipynb Lab.backup.ipynb                 # Backup your work.\n",
    "git commit -am \"My progress so far.\"          # commit your work.\n",
    "git pull upstream main --allow-unrelated-histories -X theirs # pull the updates\n",
    "```\n",
    "\n",
    "Or you can use the script we provide:\n",
    "\n",
    "```\n",
    "./fix-repo\n",
    "./pull-updates\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Be sure to save your current progress!!!\n",
    "\n",
    "Then run this cell. It'll fix your git repo history so you can successfully merge in updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./fix-repo\n",
    "!./pull-updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Then, reload this page in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## How To Use This Document\n",
    "\n",
    "You will use Jupyter Notebook to complete this lab.  You should be able to do much of this lab without leaving Jupyter Notebook.  The main exception will be some of the programming labs.  The instructions will make it clear when you should use the terminal.\n",
    "\n",
    "### Running Code\n",
    "\n",
    "Jupyter Notebooks are made up of \"cells\".  Some have Markdown-formatted text in them (like this one).  Some have Python code (like the one below).\n",
    "\n",
    "For code cells, you press `shift-return` to execute the code.  Try it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"I'm in python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Code cells can also execute shell commands using the `!` operator.  Try it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo \"I'm in a shell\""
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Telling What The Notebook is Doing\n",
    "\n",
    "The notebook will only run one cell at a time, so if you press `shift-return` several times, the cells will wait for one another.  You can tell that a cell is waiting if it there's a `*` in the `[]` to the left the cell:\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "You'll can also tell _where_ the notebook is executing by looking at the table of contents on the left.  The section with the currently-executing cell will be red:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAADcAAAA1CAYAAADlE3NNAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAycDPIMYgwyCSmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisHbud/s7f09wx77xWn4rBr1hM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rACicXg0u2h90AAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAA3oAMABAAAAAEAAAA1AAAAABCNvQ0AAAJGSURBVGgF7VkxayJBFP52XVC4YHNRxFoQRLSwvLTaWopgArkfYK2IlUViY5PzL1jZqI2dlbbaiWisREXUQnJYyF6S2WLvAgvZ3cwes8NMNTvz9r3ve9+beSwrnc/nV3A6ZE55abQEObeqK5QTyjGYAWW/3zMIiw4k6fV90HHFnhdxobCniTlEQjlzeWLPShmNRuyhooWI3JZWxnA4tGJOxdZuTHHmaFXJ//bDtXKKE9ncbDbYbremXMdiMXi9XlO2Vox2ux0cIVcqlTAej01hqVQqyOVypmw/M7pcLmg2m+h0OjgcDs6QU1X1Mxz6vqr+0edfmby8/Mbd3S0Wi4VWCfF43BlyXwFp993HxweNWCqVQr1eRzAYhJzJZJBIJEDKw0rG7YJw4r31eo1ut4tAIICnp18aMRJHbjQaqNVq6PV6GAwGTsR23OdkMtFipNNpXF190+PJpDaz2SwikQhWq5W+4aYJUY6McDj8Abbe53w+n2vLMhqNaqRms5kxuQ+rLntIJpMg4vT7fSyen3X0unL6igsnfr8fxWIRpM/9vL/X+hxpDVyQI3oUCgXk83mcTidUq1Xc3Pz42+darRY1zUKhEJbLpSl/19ffTdmZMSqXyyCtrd1uYzqdAlY/uOx+W1mN86+93ZjclKWRsoKcUVbcsCaUc4NKRhglchMZbfCwJn6EuFVFcaEI5RjMgDKfzxmERQeSdDweuW0Fiizze6coHo+HTg0w6IVv5bguS67JSZLE4GmhA4nrM8dvH3gXX+G5LN8AO2tDVwI9BI4AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### What to Do Jupyter Notebook It Gets Stuck\n",
    "\n",
    "First, check if it's actually stuck: Some of the cells take a while, but they will usually provide some visual sign of progress.  If _nothing_ is happening for more than 10 seconds, it's probably stuck.\n",
    "\n",
    "To get it unstuck, you stop execution of the current cell with the \"interrupt button\":\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC4AAAAiCAYAAAAge+tMAAABRGlDQ1BJQ0MgUHJvZmlsZQAAKJFjYGASSSwoyGFhYGDIzSspCnJ3UoiIjFJgf8rAxcDBIMjAw6CZmFxc4BgQ4ANUwgCjUcG3awyMIPqyLsisycd66uYamIqqf/v/Otp99zlM9SiAKyW1OBlI/wHitOSCohIGBsYUIFu5vKQAxO4AskWKgI4CsueA2OkQ9gYQOwnCPgJWExLkDGTfALIFkjMSgWYwvgCydZKQxNOR2FB7QYDHxdXHRyHAxNjQPJCAc0kHJakVJSDaOb+gsigzPaNEwREYSqkKnnnJejoKRgZGhgwMoDCHqP75FhyWjHWbEWKJwQwMhq1AQSGEWLYoA8Oe3wwMQrsRYlp5DAyCDQwM++MLEosS4Q5g/MZSnGZsBGGLhzEwcHb9//8C6FHuf0C79P7//8H7///vegYG9iIGhm4rAKtIXcvOgP2JAAAAOGVYSWZNTQAqAAAACAABh2kABAAAAAEAAAAaAAAAAAACoAIABAAAAAEAAAAuoAMABAAAAAEAAAAiAAAAALSq6z8AAAJiSURBVFgJ7Zi/ayJBFMe/HoLCiZ0WdrERRFDwHxBBazstPCHXCBexUHJgIZYXW3P+BzaCIFpZWSgKFwQtDCqaRsHCH4WQEyXqXd6AksQznM6uENgBnTdvmPc++93Zt8PK/jw3fMD26QMyM2QJ/Nx3Tl6tVs+d82A+hUKB5XJ5cP7VBD2ch1qlUjk0JYr/mHzSHn91G88wkBTfijwajdDv97dDQfvFYoH1es1iyoWI/PT0hGQyiVwuh+l0ykKqVCo4HA5cX3/H3d0vjMdjuN1urnT1eh2pVAqJRALc4I+Pv+HzfUGv1wOVM5PJBKVSiXa7jWw2i1KphNlsBpvNxg1OV10ulxEMBvnBb25+MGir1Yp4PA6tVstUpQu6uvoGUknoRvBcig+HQ+TzeWg0Gtze/oRK9XnHeH/fRLPZ3I2FNrjAG40G46G9/BKanHQxgUBgx2uxWHa2EAYXOClOTafT7bHo9XrQT6zGVccNBgPj6nQ6YvEdjMsFbjabWQUpFAroPTzsJZlMJqjVant+IRxc4Gq1mpUmquNfLy9ZHadqMp/PUSwW4fF44Pf7sX0WhACmGDKZjK+qUBCv14vBYIB0Oo1oNIpYLEZubDYb1rtcLhiNRmYL8UfQoVCIH5xgIpEInE4nMpkMWq0WVqsVexHZ7XbmFwKYYtALLhwOP7/wfMB7B+tjzsfvxfnfuWPyce1xoZQ8JY4EfopqPGskxXnUO2WtjJ7kUxaKseaYzxMyKlViQIgdU9rjYiv8Nr6k+FtFxB7Lu92u2Dn+Gf9lTSB7O97a29Ml9fQthfxkX1xcsHh/AeSdPWhpR3GTAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "You can also restart the underlying python instance (i.e., the confusingly-named \"kernel\" which is not the same thing as the operating system kernel) with the restart button:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Once you do this, all the variables defined by earlier cells are gone, so you may get some errors.  You may need to re-run the cells in the current section to get things to work again.\n",
    "\n",
    "You can also try reloading the web page.  That will leave Python kernel intact, but it can help with some problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Common Errors and Non-Errors\n",
    "\n",
    "1.  If you get `sh: 0: getcwd() failed: no such file or directory`, restart the kernel.\n",
    "2.  If you get `INFO:MainThread:numexpr.utils:Note: NumExpr detected 40 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.`.  It's not a real error.  Ignore it. \n",
    "3.  Sometimes `cse142 job run` will just sit there and seemingly do nothing.  Weirdly, interrupting the kernel (button above) seems to jolt it awake and cause it to continue.\n",
    "4. If you get errors similar to `NameError: name 'render_csv' is not defined`, please re-execute the very beginning cell of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### The Embedded Code\n",
    "\n",
    "The code embedded in the lab falls into two categories:\n",
    "\n",
    "1.  Code you need to edit and understand.\n",
    "2.  Code that you do not need to edit or understand -- it's just there to display something for you.\n",
    "\n",
    "For code in the first category, the lab will make it clear that you need to study, modify, and/or run the code.  If we don't explicitly ask you to do something, you don't need to.\n",
    "\n",
    "Most of the code in the second category is for drawing graphs.  You can just run it with shift-return to the see the results.  If you are curious, it's mostly written with `Pandas` and `matplotlib`. The code is all in `notebook.py`.   These cells should be un-editable.  However, if you want to experiment with them, you can copy _the contents_ of the cell into a new cell and do whatever you want (If you copy the cell, the copy will also be uneditable).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Most Cells are Immutable** Many of the cells of this notebook are uneditable. The only ones you should edit are some of the code cells and the text cells with questions in them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Answering Questions\n",
    "\n",
    "Throughout this document, you'll see some questions (like the one below).  You can double click on them to edit them and fill in your answer.  Try not to mess up the formatting (so it's easy for us to grade), but at least make sure your answer shows up clearly.  When you are done editing, you can `shift-return` to make it pretty again.\n",
    "\n",
    "A few tips, pointers, and caveats for answering questions:\n",
    "\n",
    "1. The answers are all in [github-flavored markdown](https://guides.github.com/features/mastering-markdown/) with some html sprinkled in.  Leave the html alone.\n",
    "2. Many answers require you to fill in a table, and many of the `|` characters will be missing.  You'll need to add them back.\n",
    "3. The HTML needs to start at the beginning of a line.  If there are spaces before a tag, it won't render properly.  If you accidentally add white space at the beginning of a line with an html tag on it, you'll need to fix it.\n",
    "4. Text answers also need to start at the beginning of a line, otherwise they will be rendered as code.\n",
    "5. Press `shift-return` or `option-return` to render the cell and make sure it looks good.\n",
    "6. There needs to be a blank line between html tags and markdown.  Otherwise, the markdown formatting will not appear correctly.\n",
    "\n",
    "\n",
    "You'll notice that there are three kinds of questions: \"Correctness\", \"Completeness\", and \"Optional\".  You need to provide an answer to the \"Completeness\" questions, but you won't be graded on its correctness.  You'll need to answer \"Correctness\" questions correctly to get credit.  The \"Optional\" questions are optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Grading\n",
    "\n",
    "Your grade for this lab will be based on the following components\n",
    "\n",
    "| Part                       | value |\n",
    "|----------------------------|-------|\n",
    "| Jupyter Notebook           | 50%   |\n",
    "| Programming Lab     | 48%   |\n",
    "| Post-lab survey.           | 2%    |\n",
    "\n",
    "No late work or extensions will be allowed.\n",
    "\n",
    "We will grade 5 of the \"completeness\" problems.  We will grade all of the \"correctness\" questions.\n",
    "\n",
    "You'll follow the directions at the end of the lab to submit the lab write up and the programming lab through gradescope. \n",
    "\n",
    "Please check gradescope for exact due dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Since Zen 2 is an x86 processor, you're strongly encourage to review x86 assembly before starting the rest of the lab. You may find [this link](https://www.cs.virginia.edu/~evans/cs216/guides/x86.html) useful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Thread-level parallelism in modern processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "ILP and MLP exist within a single core, and the degree of parallelism is limited by the number of instructions the CPU can issue per cycle. To get more parallelism, we need to use more CPUs, and for that we'll need to create threads.\n",
    "\n",
    "A thread is a flow of control through your program that runs on a processor. Every program has at least one thread, and by creating multiple threads you can spread the work of your program across many cores, hopefully improving it's performance.\n",
    "\n",
    "Making programs fast via multi-threading is an extremely deep and complex area. We could easily spend an entire quarter studying techniques for creating, managing, and using threads, and most universities (including UCR) offer several courses on this topic (Start with Operating Systems and then take the graduate Parallel Computation course). Indeed, some people have devoted their entire careers to the topic.\n",
    "\n",
    "All this effort is for good reason: the amount of ILP and MLP that individual cores can utilize has been roughly constant over last decade and shows no signs of improving much. Making matters worse, clock speeds are growing very slowly. That means that adding cores is the main way that computer are getting faster, but that only works if we can use threads effectively.\n",
    "\n",
    "But, it's week 8, and we don't have time for all of that. Instead, we are going to take a whirlwind tour of how you can create threads, how to make them communicate with one another, why the underlying hardware can make that hard and/or slow, and what you can do about it.\n",
    "\n",
    "To start, let's see how many cores we have (again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"lscpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "The key lines are `Socket(s): 1` and `Core(s) per socket:  4`.  A \"socket\" is place on a motherboard to stick a physical CPU.  We have 1, so all our cores live on one chip. That chip has 6 \"cores\".  A core is complete processor pipeline. \n",
    "\n",
    "You might notice that it also says `CPU(s): 8`.  This would be better phrased as \"logical cores\".  It's twice the number of actual (physical) cores because each of the cores can run two threads at once via Hyperthreading.  These cores are numbered 0-11.\n",
    "\n",
    "By convention, the lower half of the core numbers cover one logical core on each physical core.  The top half of the numbers are the second logical cores.  So, logical core 0 and logical core 6 are on the same physical core.\n",
    "\n",
    "For now, we are going to stick to logical cores 0-3.  We'll return to the upper 8 logical processors later in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Spawning Threads\n",
    "\n",
    "The first step to using threads is to create some.  C++ has pretty good threading facilities.  The key is the `std::thread` object that represents a running thread.  To start a thread, you create an `std::thread` object and pass it a function to call and the arguments you'd like to pass to the function.\n",
    "\n",
    "The `std::thread`'s `join()` method waits for the thread to complete.\n",
    "\n",
    "Here's some code that runs three threads that print out some numbers. The cell below will run the code three times separated by \"FINISHED EXECUTION\".  Pay close attention to the output of each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"threads.cpp\", function=\"threads\", opt=\"-O1\", cmdline=r\"\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "\n",
    "void go() {\n",
    "    for(int i = 0; i < 15; i++) \n",
    "        std::cerr << i << \"\\n\";\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* threads(uint64_t threads, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    std::thread t1(go);  // Create a thread to run go().  Pass no arguments.\n",
    "    std::thread t2(go);\n",
    "    std::thread t3(go);\n",
    "    \n",
    "    t1.join(); // wait for t1 to finish.\n",
    "    t2.join();\n",
    "    t3.join();\n",
    "    std::cerr << \"FINISHED EXECUTION\\n\";\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, threads);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O1\" build/threads.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/threads.so -function threads threads threads -stats threads.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "See how the order of the number changes? This is because the relative execution rate of each thread is different. Also, the threads take turns writing to standard output and the order they go in is non-deterministic.\n",
    "\n",
    "This non-determinism is the bane of multi-threaded debugging: Imagine if your bug only occurred for one of the very many possible orderings of the operations in your threads? Your bug might occur just 1 in 100 (or 1000 or 10,000) times you run the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Measuring Thread Behavior\n",
    "\n",
    "Of course, we will want to measure the performance and behavior of our threads.  Things get a little tricky here, because of a limitation of our performance counter measurement library.  It can only measure performance counters of the main thread of the program.  \n",
    "\n",
    "For the experiments we are going to run this is not a big problem: All our threads will be doing the same thing at the same time, so measuring one is a good as measuring any other.  In some cases, though, we will need to _estimate_ aggregate values across all the cores/threads.  For instance, if we want to estimate the _total_ number of instructions execute by all threads, we'll multiply the single-thread IC we measure for one thread by the thread count.\n",
    "\n",
    "Here's a simple threaded program that runs one miss chain in each of several threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = fiddle(\"threads.cpp\", function=\"threads\", opt=\"-O1\", cmdline=r\"\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include\"MissMachine.hpp\"\n",
    "\n",
    "void go(MissMachine * machine, uint64_t arg2) {\n",
    "    machine->load_miss(arg2);\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* threads(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    MissMachine a( arg1, size);\n",
    "    a.make_links();\n",
    "\n",
    "    std::thread **threads = new std::thread*[thread_count-1]; // Alloce space for some pointers\n",
    "    for(unsigned int i = 0; i < thread_count-1; i++) {\n",
    "        threads[i] = new std::thread(go, &a, arg2); // create the threads.  They will each run the miss machine.\n",
    "    }\n",
    "    go(&a, arg2); // So will this thread, hence the -1\n",
    "    for(unsigned int i = 0; i < thread_count-1; i++) { // wait for everyone else\n",
    "        threads[i]->join();\n",
    "        delete threads[i]; // cleanup\n",
    "    }\n",
    "    delete threads; // cleanup.\n",
    "    \n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, threads);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O1\" build/threads.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/threads.so -M 3300 -size 4096 --detail -f threads --arg1 8 --arg2 100000000 --threads 1 2 3 4 -o threads.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#key data_cell\n",
    "display_mono(render_csv(\"threads.csv\", columns=[\"threads\", \"size\", \"arg1\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_dcache_miss_rate\"]))\n",
    "plotPE(\"threads.csv\", lines=True, what=[(\"threads\", \"IC\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": false,
    "solution2": "hidden",
    "solution2_first": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><div class=\"question correctness\">\n",
    "\n",
    "### Question 1 (Correctness)\n",
    "\n",
    "<div class=\"question-text\">\n",
    "With 4 threads how many total instructions were executed?\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><div class=\"question correctness\">    \n",
    "<div class=\"answer\">\n",
    "    \n",
    "**Total IC**: \n",
    "    \n",
    "</div>\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Thread Communication with Volatile Variables and Locks\n",
    "\n",
    "In order for threads to work together, they must share variables:  _Sharing_ means that more than one thread is reading and/or writing to the variable during the same period of time.  Threads working together _must_ share some information, otherwise they cannot make progress together on a common goal.  For example, imagine expecting 8 people in sealed rooms, who have never met, to make progress on a single task -- it is not possible.\n",
    "\n",
    "There are two separate problems we need to solve.  The first is _how_ to share data and the second is how to share it in a coordinated and reliable way. \n",
    "\n",
    "### Sharing Data Between Threads with `volatile`\n",
    "\n",
    "The code below declares a global variable, initializes it to zero, and then provides `wait()` to wait for it to change.  It also provides `signal()` to update the global variable.  You could imagine that two threads could use these two function to coordinate in a simple way:  Thread `T1` could call `wait()` to wait for another thread, `T2`, to do something.  When `T2` is done, it could call `signal()` to let `T1` know it has done it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"not_shared.cpp\", function=\"wait\",  opt=\"-O3\", cmdline=r\"\",\n",
    "code=r\"\"\"\n",
    "\n",
    "int flag = 0;\n",
    "extern \"C\"\n",
    "void wait() {\n",
    "    while(flag);\n",
    "}\n",
    "\n",
    "void signal() {\n",
    "    flag = 1;\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "! g++ -S -O3 build/not_shared.cpp\n",
    "render_code(\"not_shared.s\", show=[\"wait:\", \".LFE0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "As written and compiled with optimizations, the compiler does what we would expect:  it checks `flag` once. If it's non-zero (i.e., it evaluates to `true`), it returns.  Otherwise it loops infinitely.  This will clearly not work as a thread communication mechanism:  Unless `T2` calls `signal()` _before_ `T1` calls `wait()`, `T1` will never get the message.\n",
    "\n",
    "The compiler is assuming that `flag` _will not change_.  This is a valid assumption for the compiler to make because, by default, variables in C and C++ are considered to the thread-private -- only the current thread will access them.  That's not what we want for thread communication.\n",
    "\n",
    "We can fix this by declaring `flag` as `volatile`.  `volatile` tells the compiler that the variable is shared and so it might change at any time, which dramatically reduces the number of optimizations it can apply.  Let's see what it does now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"not_shared.cpp\", function=\"wait\", opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "\n",
    "volatile int flag = 0;\n",
    "extern \"C\"\n",
    "void wait() {\n",
    "    while(flag);\n",
    "}\n",
    "\n",
    "void signal() {\n",
    "    flag = 1;\n",
    "}\n",
    "\n",
    "\"\"\", run=None)\n",
    "! g++ -S -O3 build/not_shared.cpp\n",
    "render_code(\"not_shared.s\", show=[\"wait:\", \".LFE0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Now, `wait()` checks `flag` every time, and our communication mechanism will work.  Or will it...\n",
    "\n",
    "### Memory Ordering\n",
    "\n",
    "Here's a slightly more complicated example that actually uses our communication mechanism: instead of waiting on a fixed value for flag, `wait()` will wait for a configurable value.  The threads take turns waiting on one another, and set `other_value` each time.  They also check whether `flag` and `other_value` match, and tell us if they don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"toggle.cpp\", function=\"toggle\", opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "\n",
    "volatile int flag = 0;\n",
    "volatile int other_value = 0;\n",
    "\n",
    "extern \"C\"\n",
    "void wait(int k) {\n",
    "    while(flag != k);\n",
    "}\n",
    "\n",
    "void signal(int k) {\n",
    "    flag = k;\n",
    "}\n",
    "\n",
    "void play(int my_id, int other_id, int count) {\n",
    "    for(int i = 0; i < count; i++) {\n",
    "        wait(other_id);\n",
    "        int t_flag = flag;\n",
    "        int t_other_value = other_value;\n",
    "        if (t_flag != t_other_value) {\n",
    "            std::cerr << \"Mismatch: \" << t_flag << \" != \" << t_other_value << \"\\n\";\n",
    "        }\n",
    "        other_value = my_id;\n",
    "        signal(my_id);\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* toggle(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    std::thread T0(play, 0, 1, 10000000);\n",
    "    std::thread T1(play, 1, 0, 10000000);\n",
    "    \n",
    "    T0.join();\n",
    "    T1.join();\n",
    "    return data;\n",
    "}\n",
    "\n",
    "FUNCTION(one_array_2arg, toggle);\n",
    "\"\"\", run=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": false,
    "solution2": "hidden",
    "solution2_first": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><div class=\"question completeness\">\n",
    "\n",
    "### Question 2 (Completeness)\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "In the above code, will the `std::cerr` line ever execute?  Why or why not?\n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"answer\">\n",
    "Answer:     \n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "We start off with `flag == 0` and `other_value == 0`.\n",
    "    \n",
    "`T0` will spend some time `wait()`ing for 1 (`other_id`).\n",
    "    \n",
    "`T1` will call `wait()` for 0 (its value of `other_id`), but will not wait at all, since `flag` was initialized to 0.  `T1` will read `flag` and `other_value`.  They will both be 0, and the `std::cerr` will not happen.\n",
    "    \n",
    "`T1` then sets `other_value` to 1 (it's value of `my_id`).  *After* that, it calls `signal()` which sets `flag` to 1.\n",
    "    \n",
    "At this point, it seems that `T0`'s call to `wait()` will find that `flag == 1`, and stop waiting.  Then `T0` will the do the same things that `T1` did, as described above.\n",
    "\n",
    "It would seem that since each thread sets `other_value` before calling `signal()`, that the `std::cerr` should never execute.\n",
    "    \n",
    "But of course, then, why did I write this code and this question?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Let's run it and see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fiddle(\"toggle.cpp\", function=\"toggle\", opt=\"-O3\", run=[\"perf_count\"])\n",
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O3\" build/toggle.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/toggle.so -M 3300 --detail -f toggle  -o toggle.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Hmmm... That is definitely not \"never\" executing, although it is pretty rare.  It's also non-deterministic:  If you run it more than once, you'll get different mismatches.\n",
    "\n",
    "What's going on?  It would seem that our intuitive understanding of how our multi-threaded program should execute does not match what actually happens when it really executes.\n",
    "\n",
    "This source of this mismatch is something called the processor's _consistency model_.   The consistency model determines how processors \"see\" memory operations performed by other processors.  In particular, it  determines in what order stores _appear_ to occur.  Different instruction sets come with different consistency models: x86 has one, ARM has one, etc.\n",
    "\n",
    "Consistency models are easily the most complicated aspect of modern Instruction Set Architectures, and their details are pretty far beyond the scope of this course. However, they are crucial for making multithreaded programs work, so we will cover a few basic points.\n",
    "\n",
    "The basic problem with the code above is that the x86 consistency model allows the stores that `T0` performs to _appear_ , from `T1`'s perspective, to happen in a different order than `T0` executed them in.  In the code above, `T1` can load from `flag` and find that it's equal to 0 _and then_ read from `other_value` and find it equal to one.\n",
    "\n",
    "x86 provides some special instructions that prevent this reordering.  They are called 'fences' because they keep memory operations from moving around.  It's possible to use them directly, but that's a subject for another class.  Instead, we'll use _locks_, which are a cleaner way of coordinating threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "###  Locks\n",
    "\n",
    "A _lock_ or _mutual exclusion variable (mutex)_ is a small data structure that can be _locked_ and _unlocked_.  We'll use C++'s `std::mutex`.  \n",
    "\n",
    "If a thread calls `lock()` on a mutex that is not currently locked, the thread \"holds\" the lock and starts executing a region of code called a \"critical section\".  At the end of the critical section, it calls `unlock()` to release the lock.\n",
    "\n",
    "If a thread, `T`, calls `lock()` on a mutex that _is_ currently locked, it will wait until the thread that holds it calls `unlock()`.  Then, `T` gets the lock and can proceed.\n",
    "\n",
    "The result is that, at any time, only one thread is executing inside the critical section.\n",
    "\n",
    "Internally, locks are implementing using some kind of flag (similar to `flag` in our example above) and some of those fences I mentioned above. \n",
    "\n",
    "Here's an example.  In this code, we have a shared variable `shared`.  Several threads are going to work together to increment `shared` 600,000 times.  If we run with $n$ threads, each will do 600,000/$n$ increments.\n",
    "\n",
    "If `do_lock` is `true`, they will use `yes_locks()` which protects each lock with an increment.  Otherwise, they will use `no_locks()` which doesn't.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = fiddle(\"lock_demo.cpp\", function=\"lock_demo\", opt=\"-O1\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<mutex>\n",
    "\n",
    "std::mutex lock;\n",
    "volatile int shared = 0;\n",
    "extern \"C\"\n",
    "void yes_locks(uint64_t id, int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        lock.lock();\n",
    "        shared++;\n",
    "        lock.unlock();\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "void no_locks(uint64_t id, int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        shared++;\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* lock_demo(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    shared = 0;\n",
    "    std::thread **threads = new std::thread*[thread_count];\n",
    "    bool do_locks = (arg2 != 0);\n",
    "    for(unsigned int i = 0; i < thread_count - 1; i++) {\n",
    "        if (do_locks) {\n",
    "            threads[i] = new std::thread(yes_locks, i, arg1/thread_count);\n",
    "        } else {\n",
    "            threads[i] = new std::thread(no_locks, i, arg1/thread_count);\n",
    "        }\n",
    "    }\n",
    "    if (do_locks) {\n",
    "        yes_locks(thread_count - 1, arg1/thread_count);\n",
    "    } else {\n",
    "        no_locks(thread_count - 1, arg1/thread_count);\n",
    "    }\n",
    "    for(unsigned int i = 0; i < thread_count - 1; i++) {\n",
    "        threads[i]->join();\n",
    "    }\n",
    "    std::cerr << \"do_lock: \" << do_locks << \"; \" << \"thread count: \" << thread_count << \"; Shared sum: \" << shared << \".  It should be: \" <<  arg1 << \"\\n\";\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, lock_demo);\n",
    "\"\"\")\n",
    "\n",
    "!make C_OPTS=\"-O1\" build/lock_demo.s\n",
    "render_code(\"lock_demo.s\", show=[\"yes_locks:\",\"LFE3782\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O1\" build/lock_demo.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/lock_demo.so -M 3300 --detail -f lock_demo --arg1 60000000 --arg2 1 0 --threads 1 2 3 4 -o lock_demo.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": false,
    "solution2": "hidden",
    "solution2_first": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><div class=\"question completeness\">\n",
    "\n",
    "### Question 3 (Completeness)\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Based on the assembly above, and assuming multiple threads are running at once, explain how `shared` ends up being computed incorrectly without locks and how adding locks prevents it.\n",
    "  \n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">    \n",
    "<div class=\"answer\">\n",
    "Answer:  \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "The increment turns into 3 instruction:  `mov (%rcx), %eax` (a load), `addl $1, %eax` (the add), and `movl %eax, (%rcx)` (a store).\n",
    "\n",
    "The problem comes when the load in two threads retrieves the same value.  They then compute the same next value and store it back.  As a result one of the increments is lost.\n",
    "  \n",
    "Adding `lock()` and `unlock()` avoids this problem since only one thread is executing the increment at a time.  So no increments are lost.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "So locks are fine for correctness (which is very important), but what about performance.\n",
    "\n",
    "Here's a bunch of data and graphs about the performance of the code above with and without threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deleteable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#key data_cell\n",
    "from notebook import *\n",
    "df = render_csv(\"lock_demo.csv\")\n",
    "df[\"label\"] = df[\"threads\"].apply(lambda x: f\"{x} threads;\" ) + \" \" + df[\"arg2\"].apply(lambda x: \"locks\" if x else \"no locks\")\n",
    "df[\"Total IC\"] = df[\"IC\"] * df[\"threads\"]\n",
    "df[\"IC per increment\"] = df[\"IC\"]/(df[\"arg1\"]/df[\"threads\"])\n",
    "df[\"Cycles per increment\"] = df[\"Cycles\"]/(df[\"arg1\"]/df[\"threads\"])\n",
    "df[\"L1 Misses Per Increment\"] = df[\"L1_dcache_misses\"]/(df[\"arg1\"]/df[\"threads\"])\n",
    "df[\"locks\"] = df[\"arg2\"]\n",
    "display_mono(df[[\"threads\", \"locks\", \"IC per increment\", \"CPI\", \"ET\", \"Cycles per increment\", \"L1 Misses Per Increment\"]])\n",
    "plotPEBar(df=df, what=[(\"label\", \"IC per increment\"), (\"label\", \"CPI\"), (\"label\", \"ET\"),  (\"label\", \"Cycles per increment\"), (\"label\", \"L1 Misses Per Increment\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 3,
    "cse142.question_type": "correctness",
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"><div class=\"question correctness\"> \n",
    "\n",
    "### Question 4 (Correctness)\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Answer the questions below:\n",
    "    \n",
    "* How much does adding locks slow down the single thread case in terms of cycles per increment? \n",
    "    \n",
    "* How much does adding a second thread slow down each increment in terms of cycles per increment? \n",
    "    \n",
    "* How many cycles does it take to take and release a lock? \n",
    "    \n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 3,
    "cse142.question_type": "correctness",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"answer\">\n",
    "\n",
    "* How much does adding locks slow down the single thread case in terms of cycles per increment? \n",
    "    \n",
    "* How much does adding a second thread slow down each increment in terms of cycles per increment? \n",
    "    \n",
    "* How many cycles does it take to take and release a lock? \n",
    "\n",
    "</div>\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "There are two main things to take away from this data:\n",
    "\n",
    "1.  Just taking and releasing locks is expensive -- even if there's only one thread.  This overhead comes mostly from increased instruction count.  In addition, fences are expensive: could take hundreds of cycles on our machine.\n",
    "2.  When there is more than one thread competing for the lock, things get even worse.  This overhead comes from increase cache misses.\n",
    "\n",
    "Both of these are noteworthy.  The extra instructions in the locks are an example of the overhead involved in improving performance.  We saw this before with loop tiling:  Splitting and renesting loops can improve performance but it also increases instruction count, so some of the improved performance goes to paying for that overhead.  It the same thing here:  Taking and releasing a lock is code we didn't have to run before and that doesn't contribute to the useful work our program is doing.  There is no free lunch.\n",
    "\n",
    "The cache misses are a bigger problem and, interestingly, they are due to kind of cache miss we have not seen before in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Cache Coherence and the 4th C\n",
    "\n",
    "In lab 2 we've seen the Capacity, Conflict, and Compulsory misses, but there is a 4th kind of miss that only occurs in multi-processing systems:  Coherence Misses.\n",
    "\n",
    "As you learned in CSE142, cache coherence is how the processor keeps multiple caches synchronized, so that the processors all see the same value for a given address.\n",
    "\n",
    "To refresh your memory, the key point of coherence are:\n",
    "\n",
    "1.  Coherence operates on cache lines, like all things in the memory hierarchy.\n",
    "2.  Multiple processors can have a copy of the a cache line in their cache if they are _only_ reading from it.\n",
    "3.  Only a single processor may have a copy of the cache line if it is writing to it.\n",
    "\n",
    "Enforcing #2 and #3 is expensive:  When a processor wants to update a cache line, it has to tell all the processors that have a copy of it to _invalidate_ their copy.  This means removing it from the cache.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "When a memory access would have been a hit, but it is a miss because the cache was invalidated, we call that a _coherence miss_.\n",
    "\n",
    "Satisfying cache misses is multiprocessors is also more complicated due to coherence:  If a load misses in the cache it has to check the other caches to see if they have a copy.  If they do, then that copy is more up-to-date than what is in main memory, so that is where the cache line needs to be loaded from.  This is called a _cache-to-cache_ transfer.\n",
    "\n",
    "The invalidations and cache-to-cache transfers are implemented by sending message between the caches over an on-chip network. \n",
    "\n",
    "You may reference Figure 3 of the Zen 2 paper as an example.\n",
    "\n",
    "Sending messages across such a network can be pretty expensive and take quite a long time.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Coherence Performance\n",
    "\n",
    "Here's a simple program to measure the cost of communicating between cores.  We are going to run two threads either on the same core or on two different cores.  The only trick is that we control which thread runs where.  You can look at `threads.hpp` to see how `bind_to_core()` works.\n",
    "\n",
    "One thing to keep in mind:  When the two threads are running on the same core, they will have to take turns, so that single core will be doing twice as much work than each core in the two-core case.\n",
    "\n",
    "So, in an ideal world, the two-core case would be twice as fast as the one-core case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = fiddle(\"coherence.cpp\", function=\"coherence\", opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<mutex>\n",
    "#include \"threads.hpp\"\n",
    "#include \"pthread.h\"\n",
    "\n",
    "std::mutex lock;\n",
    "volatile int shared = 0;\n",
    "void go(uint64_t id,int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        lock.lock();\n",
    "        shared++;\n",
    "        lock.unlock();\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* coherence(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    shared = 0;\n",
    "    std::thread other (go, 1, arg1);\n",
    "    bind_to_core(other, arg2);\n",
    "\n",
    "    bind_to_core(pthread_self(), arg3);\n",
    "    go(0, arg1);\n",
    "    other.join();\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, coherence);\n",
    "\"\"\")\n",
    "\n",
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O3\" build/coherence.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/coherence.so -M 3300 --detail -f coherence --arg1 10000000 --arg2 0 1   --arg3 0 -o coherence.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"coherence.csv\")\n",
    "df[\"other_core\"] = df[\"arg2\"]\n",
    "df[\"this_core\"] = df[\"arg3\"]\n",
    "df[\"label\"] = df[\"arg2\"].astype(str) + \" to \" + df[\"arg3\"].astype(str) \n",
    "df[\"IC per increment\"] = df[\"IC\"]/df[\"arg1\"]\n",
    "df[\"Cycles per increment\"] = df[\"Cycles\"]/df[\"arg1\"]\n",
    "display_mono(df[[\"threads\", \"size\", \"arg1\", \"this_core\", \"other_core\", \"arg3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_dcache_miss_rate\"]])\n",
    "plotPEBar(df=df,  what=[(\"label\", \"CPI\"), (\"label\", \"ET\"), (\"label\", \"L1_dcache_misses\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "In this case, two cores is not better than one:  Even with two processors to do the work, execution slows down by a wide margin!\n",
    "\n",
    "The underlying problem is all the coherence misses necessary as `shared`'s cache line \"ping pongs\" between the two cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = fiddle(\"false_sharing.cpp\", function=\"false_sharing\", opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<mutex>\n",
    "#include\"threads.hpp\"\n",
    "#include\"pthread.h\"\n",
    "\n",
    "volatile int shared = 0;\n",
    "volatile int not_shared_0 = 0;\n",
    "volatile int not_shared_1 = 0;\n",
    "\n",
    "void go_0(uint64_t id,int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        not_shared_0++;\n",
    "    }\n",
    "}\n",
    "\n",
    "void go_1(uint64_t id,int count) {\n",
    "    for(int i= 0; i < count; i++){\n",
    "        not_shared_1++;\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* false_sharing(uint64_t thread_count, uint64_t * data, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    shared = 0;\n",
    "    std::thread other (go_0, 1, arg1);\n",
    "    bind_to_core(other, arg2);\n",
    "\n",
    "    bind_to_core(pthread_self(), arg3);\n",
    "    go_1(0, arg1);\n",
    "    other.join();\n",
    "    shared = not_shared_0 + not_shared_1;\n",
    "    return data;\n",
    "}\n",
    "FUNCTION(one_array_2arg, false_sharing);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O3\" build/false_sharing.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/false_sharing.so -M 3300 --detail -f false_sharing --arg1 10000000 --arg2 0 1   --arg3 0 -o false_sharing.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"question completeness\">\n",
    "\n",
    "### Question 5 (Completeness)    \n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "How much difference in performance do you expect to see between running both threads on one core vs. running them on two cores?\n",
    "    \n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<div class=\"answer\">\n",
    "    \n",
    "Answer:\n",
    "    \n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cse142.is_response": true,
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"false_sharing.csv\")\n",
    "df[\"other_core\"] = df[\"arg2\"]\n",
    "df[\"this_core\"] = df[\"arg3\"]\n",
    "df[\"label\"] = df[\"arg2\"].astype(str) + \" to \" + df[\"arg3\"].astype(str) \n",
    "df[\"IC per increment\"] = df[\"IC\"]/df[\"arg1\"]\n",
    "df[\"Cycles per increment\"] = df[\"Cycles\"]/df[\"arg1\"]\n",
    "display_mono(df[[\"threads\", \"size\", \"arg1\", \"other_core\", \"arg3\", \"IC\", \"CPI\", \"CT\", \"ET\", \"L1_dcache_miss_rate\"]])\n",
    "plotPEBar(df=df,  what=[(\"label\", \"CPI\"), (\"label\", \"ET\"), (\"label\", \"L1_dcache_misses\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Interesting.  That helped some, but not as much we'd like:  Ideally, we'd get 2x speedup with 2 cores, but instead we only get about 1.4x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<div class=\"question completeness\">\n",
    "\n",
    "### Question 6 (Completeness)    \n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Add a single line to the code above to get the 2x performance improvement we seek. (Hint:  The memory system thinks in cache lines and so should you).\n",
    "    \n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">       \n",
    "<div class=\"answer\">\n",
    "Answer:\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "    \n",
    "The program above has no sharing in it, but it's written in terms of variables.  The memory hierarchy doesn't share variables, though, it shares cache lines.\n",
    "\n",
    "The problem here is that `not_shared_0` and `not_shared_1` reside in the same cache line, and that cache line is shared between the two processors, so there's still a lot of (now totally useless) communication going on between the two caches.\n",
    "    \n",
    "The solution is to add some padding between the those two variables.  An array of 8 `uint64_t`s should do it.  That will push them into two different cache lines.  Give it a try.\n",
    "   \n",
    "This phenomenon is called \"false sharing\" and it's quite common.  A great way to avoid false sharing among small objects is to make them cache line-aligned.  If only we had a way to control the alignment of memory objects we allocate...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Non-Uniform Memory Access\n",
    "\n",
    "Multiple processors complicates the notion of memory latency as well. \n",
    "\n",
    "If you look closely at the on-chip network above, you can notice that there are two memory controllers -- one on each of the side of the on-chip network.  This mean that, depending on where a core is in relation to the DRAM it's trying to access, the memory latency will be different.  Similarly, depending on which part of the L3 cache a cache line landed, the latency of an L2 cache might vary.\n",
    "\n",
    "This effect is called non-uniform memory access (NUMA).  NUMA effects are even greater if a computer has multiple sockets -- some memory requests will have to go between chips while others are \"local\".\n",
    "\n",
    "It'd be great to measure NUMA effects on our machine, but our machine only has 6 cores, 1 socket, and one (active) memory controller, so there's not enough non-uniformity to measure reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Example: Histogram\n",
    "\n",
    "Let's apply what we have learned so far to implement a fast histogram.  A histogram counts the number of occurrences of data values in a sample of data.  In our case, we are going to count how often each of the 256 possible 1-byte values appears in an array of 64-bit values.  So our histogram will have 256 \"buckets\".  Our task is to compute the histogram as fast as possible.\n",
    "\n",
    "\n",
    "## Serial Histogram\n",
    "\n",
    "Here's a simple serial version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=\"unthreaded_histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Run the cell below to see the assembly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! g++ -S -O3 histogram.cpp\n",
    "render_code(\"histogram.s\", show=[\"unthreaded_histogram\",\"LFE3778:\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "The compiler has completely unrolled the inner loop and performs one increment for each byte in the 64-byte value.  Let's see how fast it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_size=10000000\n",
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O3\" build/histogram.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_unthreaded_histogram --size {hist_size} --thread 0 -o histogram_unthreaded.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "display_mono(hist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "correctness",
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<div class=\"question correctness\">\n",
    "\n",
    "### Question 7 (Correctness)    \n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "Would this code benefit from tiling as a locality optimization?  Why or why not?\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "correctness",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">    \n",
    "<div class=\"answer\">\n",
    "Answer:\n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Parallel Histogram\n",
    "\n",
    "Here's a simple threaded version.  It divides the input array into chunks and processes them in parallel.  We use a single lock to protect a shared set of buckets.\n",
    "\n",
    "This is very similar to what we did with tiling for cache locality, but here our focus is on dividing up the work across threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_THREADED\", \"//END_THREADED\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<div class=\"question completeness\">\n",
    "    \n",
    "### Question 8 (Completeness)\n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "How much speedup do you expect with 4 threads? Why?\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">    \n",
    "<div class=\"answer\">\n",
    "\n",
    "**Speedup:**\n",
    "\n",
    "**Why:**\n",
    "    \n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Run the two cells below to see how it performs.  Spoiler:  this takes a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O3\" build/histogram.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_threaded_histogram --size {hist_size} --threads 1 2 3 4 -o histogram_threaded.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_threaded.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"threads\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_dcache_misses\"]*hist_data[\"threads\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"threads\", \"speedup\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "The 0 threads data point is the baseline serial version.\n",
    "\n",
    "We have certainly not improved things:  The 1-threaded threaded version is about 40x slower than the unthreaded version.  With 4 threads, it's about 200x slower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Finer-Grain Locks\n",
    "\n",
    "One problem is that we only have one lock even though we have 256 different buckets.  This means that each thread has to lock _all_ the buckets for each increment.  That's a lot of overhead and a lot of contention.  Let's fix by making our locks more \"fine grained\":  Each of them will cover just one bucket.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_FINE\", \"//END_FINE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Run these two cells to see performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_fine_locks_histogram --size {hist_size} --threads 1 2 3 4 -o histogram_fine_locks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_fine_locks.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"threads\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_dcache_misses\"]*hist_data[\"threads\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"threads\", \"speedup\"), ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "On the plus side, performance goes up after 2 threads, and 4 threads is slightly faster than 2 threads.   On the down side, 4 threads is still 33x slower than the unthreaded case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Private Histograms\n",
    "\n",
    "We still have a lot of sharing.  Let's get rid of it by giving each thread its own set of buckets.  We'll allocate an array of `thread_count * 256` buckets and compute the index based on the thread's number and the byte's value.  We can also get rid of locks because there's no more sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_PRIVATE\", \"//END_PRIVATE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Run these two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_private_histogram -s {hist_size} --threads 1 2 3 4 -o histogram_private_locks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_private_locks.csv\"], columns=hist_columns)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"threads\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_dcache_misses\"]*hist_data[\"threads\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"threads\", \"speedup\"), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Things are better still.\n",
    "\n",
    "* 1 thread is only a little slower than the unthreaded version (because we have no locks).\n",
    "* 4 threads is 1.8x faster than 2 threads, which is pretty good since the best we could do is 2x.\n",
    "\n",
    "The drop at 2 threads is a problem, though because we never recover from it.\n",
    "\n",
    "## Private Histograms, Take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 1,
    "cse142.question_type": "completeness",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<div class=\"question completeness\">\n",
    "\n",
    "### Question 9 (Completeness)    \n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "What causes the drop in performance from 1 thread to 2?  How can we fix it?\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "It's false sharing due to this calculation:  `b*thread_count + id`\n",
    "    \n",
    "Computing the bucket to use this way arranges the buckets like this:\n",
    "    \n",
    "| 0 | 1 | 2 |3 |\n",
    "|--|--|--|--|\n",
    "|thread 0; byte == 0| thread 1; byte == 0| thread 2; byte == 0|...|--|--|\n",
    "\n",
    "Which puts buckets for multiple threads into the same cache line.\n",
    "\n",
    "We'd rather have this\n",
    "    \n",
    "| 0 | 1 | 2 |3 |\n",
    "|--|--|--|--|\n",
    "|thread 0; byte == 1| thread 0; byte == 1| thread 0; byte == 2|...|--|--|\n",
    "\n",
    "Which we can achieve with `id*256 + thread_count`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_PRIVATE2\", \"//END_PRIVATE2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Let's see how that does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_private2_histogram -s {hist_size} --threads 1 2 3 4 -o histogram_private2_locks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_private2_locks.csv\"], columns=hist_columns+[\"Cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"threads\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_dcache_misses\"]*hist_data[\"threads\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"threads\", \"speedup\"), ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Finally!  That's speedup! We've got almost 1.58x speedup with 4 threads!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Amdahl's Law and Imperfect Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "In our histogram example, we finally got some speedup with threads.  Speeding up a program by $p$ with $p$ processors is called _linear speedup_ and it's always the goal of multi-threaded.  It's hard to achieve, however, because of Amdahl's Law.\n",
    "\n",
    "So far, in this class we've talked about Amdahl's Law as it applies optimizations:  The more widely applicable an optimization, the larger it's benefit.  Originally, however, Amdahl's Law was just about parallel computation.  In particular, if we have $p$ processors, Amdahl's Law bounds the maximum speedup, $S$, we can achieve:\n",
    "$$S \\leq \\frac{1}{x/p + (1-x)}$$\n",
    "\n",
    "Where $x$ is the fraction of the program that can parallelized across all $p$ processors.  For the histogram example, $x = 1$, so $S \\leq p$, and our implementation achieved this upper bound.\n",
    "\n",
    "Usually, however, $x < 1$, so $S < p$.\n",
    "\n",
    "To see this, let's parallelize merge sort.  The implementation below divides the array in half and recursively calls merge sort on each sub array.  To parallelize it, we will spawn a thread to sort each sub-array.  As the sub arrays get smaller and smaller, two things will happen:\n",
    "\n",
    "1.  We will spawn an enormous number of threads.\n",
    "2.  Each of them will do very little work.\n",
    "\n",
    "Neither of these is good, so we'll use `threshold` to control the minimum size for which we will spawn a thread.  If the sub-array is smaller than `threshold`, we'll just do all the work in the current thread.\n",
    "\n",
    "This means that if the array is of size `threshold` $ \\times 2^k$, we'll use a maximum of $2^k$ threads.\n",
    "\n",
    "You'll notice that there are no locks.  This is because there's not actually any sharing:  Only one thread is ever working on any one part of the array at a time.  (The attentive reader will recall that I said that sharing is necessary to coordinate threads.  Here, the sharing and coordination happen before the threads are created and after we `join()` them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"merge_sort.cpp\", function=\"merge_sort\", opt=\"-Og\",\n",
    "code=r\"\"\"\n",
    "//  From https://codereview.stackexchange.com/questions/87085/simple-comparison-of-sorting-algorithms-in-c\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "#include<thread>\n",
    "#include<math.h>\n",
    "#include<mutex>\n",
    "#include\"threads.hpp\"\n",
    "#include\"pthread.h\"\n",
    "\n",
    "void merge(uint64_t *list, int64_t p, int64_t q, int64_t r)\n",
    "{\n",
    "//n1 and n2 are the lengths of the pre-sorted sublists, list[p..q] and list[q+1..r]\n",
    "\tint64_t n1=q-p+1;\n",
    "\tint64_t n2=r-q;\n",
    "//copy these pre-sorted lists to L and R\n",
    "\tuint64_t * L = new uint64_t[n1+1];\n",
    "\tuint64_t * R = new uint64_t[n2+1];\n",
    "\tfor(int64_t i=0;i<n1; i++)\n",
    "\t{\n",
    "\t\tL[i]=list[p+i];\n",
    "\t}\n",
    "\tfor(int64_t j=0;j<n2; j++)\n",
    "\t{\n",
    "\t\tR[j]=list[q+1+j];\n",
    "\t}\n",
    "\n",
    "\n",
    "//Create a sentinal value for L and R that is larger than the largest\n",
    "//element of list\n",
    "\tuint64_t largest;\n",
    "\tif(L[n1-1]<R[n2-1]) largest=R[n2-1]; else largest=L[n1-1];\n",
    "\tL[n1]=largest+1;\n",
    "\tR[n2]=largest+1;\n",
    "\n",
    "//Merge the L and R lists\n",
    "\tint64_t i=0;\n",
    "\tint64_t j=0;\n",
    "\tfor(int64_t k=p; k<=r; k++)\n",
    "\t{\n",
    "\t\tif (L[i]<=R[j])\n",
    "\t\t{\n",
    "\t\t\tlist[k]=L[i];\n",
    "\t\t\ti++;\n",
    "\t\t} else\n",
    "\t\t{\n",
    "\t\t\tlist[k]=R[j];\n",
    "\t\t\tj++;\n",
    "\t\t}\n",
    "\t}\n",
    "    delete L;\n",
    "    delete R;\n",
    "}\n",
    "\n",
    "void merge_sort_aux(uint64_t *list, int64_t p, int64_t r, int64_t threshold)\n",
    "{\n",
    "\tif(p<r)\n",
    "\t{\n",
    "        int64_t q=floor((p+r)/2);\n",
    "        if (r - p > threshold) {\n",
    "            std::thread left(merge_sort_aux, list,p,q, threshold);\n",
    "            std::thread right(merge_sort_aux, list,q+1,r, threshold);\n",
    "            left.join();\n",
    "            right.join();\n",
    "        } else {\n",
    "            merge_sort_aux(list,p,q, threshold);\n",
    "            merge_sort_aux(list,q+1,r, threshold);\n",
    "        }\n",
    "\t\tmerge(list,p,q,r);\n",
    "\t}\n",
    "\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* merge_sort(uint64_t thread_count, uint64_t *list, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "\tmerge_sort_aux(list, 0, size - 1, arg1);\n",
    "\treturn list;\n",
    "}\n",
    "FUNCTION(one_array_2arg, merge_sort);\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.question_type": "optional",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<div class=\"question completeness\">\n",
    "\n",
    "### Question 10 (Completeness) \n",
    "\n",
    "<div class=\"question-text\">\n",
    "\n",
    "What is $x$ for parallel merge sort?  Can you bound speedup for 4 processors, a threshold of 1024, and a total array size of 4096?  If you can't get a precise answer try to estimate or provide an upper bound the value of $x$.\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<div class=\"answer\">\n",
    "\n",
    "    \n",
    "</div>\n",
    "        \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "solution2": "hidden"
   },
   "source": [
    "<div class=\"solution\">\n",
    "\n",
    "It's pretty tricky to calculate $x$ precisely, but we can do it big-O style.  The key observation is that merge sort in $O(n \\lg n)$ and the final merge operation is completely serialized.  That final merge takes $O(n)$ comparisons and labs, so it takes $$\\frac{n}{n\\log n}$$ of the the total execution time.\n",
    "  \n",
    "So $x$ is something like:\n",
    "\n",
    "$$ x = 1 - \\frac{n}{n\\lg n} =  1 - \\frac{1}{\\lg n}$$\n",
    "    \n",
    "This tells that as $n$ get's really big, merge sort slowly approach being perfectly parallelizable (i.e., $x = 1$).  It won't ever get there, though.\n",
    "\n",
    "Let's plug it into Amdahl's law:\n",
    "    \n",
    "\n",
    "$$S \\leq \\frac{1}{\\frac{1 - \\frac{1}{\\lg n}}{p} + (1-(1 - \\frac{1}{\\lg n}))}$$\n",
    "\n",
    "And we have, $p = 4$, $n = 4$, so:\n",
    "\n",
    "$$S \\leq \\frac{1}{\\frac{1 - \\frac{1}{\\lg 4096}}{4} + (1-(1 - \\frac{1}{\\lg 4096}))}$$\n",
    "$$S \\leq \\frac{1}{\\frac{0.92}{4} + (1-(0.92))}$$\n",
    "$$S \\leq 3.2$$\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Given our implementation, we can't directly control the number of threads we are using, but by setting `threshold` to smaller values, we'll use more threads.  In particular, if we have `size == threshold * 2^k`, we will use a peak of `size/threshold` threads, that's what `approx threads` measures.\n",
    "\n",
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O3\" build/merge_sort.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/merge_sort.so -M 3300 --detail -f merge_sort -s {8*1024*1024} --arg1 {8*1024*1024} {4*1024*1024} {2*1024*1024} {1024*1024} -o merge_sort.csv \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"merge_sort.csv\")\n",
    "df[\"i\"] = df.index\n",
    "df[\"threshold\"] = df[\"arg1\"]\n",
    "df[\"approx threads\"] = df[\"size\"]/df['threshold']\n",
    "df[\"speedup\"] = df.iloc[0][\"ET\"]/df[\"ET\"]\n",
    "display_mono(df[[\"size\", \"threshold\", \"approx threads\", \"ET\", \"speedup\"]])\n",
    "plotPE(df=df, lines=True, what=[(\"approx threads\", \"speedup\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Speedup looks ok, but it's not  \"linear speedup\".  From 1 thread to 2, we get 1.9x, but from 2 to 4, we only get 1.7x.  From 4 to 8, we get 1.25x (although, in fairness, we only have 4 cores).\n",
    "\n",
    "Unfortunately, this how it usually goes:  Linear speedup is very hard to achieve in practice, because it requires reducing communication and serialized computation to nearly zero.  We were able to do that with the histogram, but sorting (and most other things) are more complex.\n",
    "\n",
    "And it's not just that $x$ is less than 1.  There is also usually some overhead from locks (which was absent in our histogram) which further degrades performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Hyperthreading\n",
    "\n",
    "Earlier in the lab, we saw that our CPU _says_ it has 8 processors even though it really only has 4.  The 4 extra, logical processors are due to hyperthreading, a clever trick that allows two threads to run _at exactly the same time_ on the same processor.\n",
    "\n",
    "The catch is that the two threads running on the same core compete with each other for resources.  This works fine if the two threads need different resources, but if they need the same resources, performance will suffer.\n",
    "\n",
    "Let's see how it does for our two parallel programs.  Here's the histogram with threads turned up to 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_private2_histogram --size 10000000 --threads 1 2 3 4 5 6 7 8 -o histogram_hyper.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_hyper.csv\"], columns=hist_columns+[\"Cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"threads\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_dcache_misses\"]*hist_data[\"threads\"]\n",
    "display(hist_data[hist_columns])\n",
    "plotPE(df=hist_data, lines=True, what=[(\"threads\", \"speedup\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "For thread counts 5-8 some of the threads are sharing a core.  As you can see, there's a hit to performance when we go to 5 cores.  This because the execution time of the whole program is the execution time of the _slowest_ thread.  In this case, that is either thread 1 or thread 5, since they share a core.\n",
    "\n",
    "Adding more hyperthreads improves performance but not as much as adding more full processors.  For instance, doubling the number of logical processors from 4 to 8 does not give us speedup, while going from 2 to 4 gave us 2x.\n",
    "\n",
    "And here's merge sort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"make fiddle.exe; make C_OPTS=\"-O3\" build/merge_sort.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/merge_sort.so -M 3300 --detail -f merge_sort --size {8*1024*1024} --arg1 {8*1024*1024} {4*1024*1024} {2*1024*1024} {1024*1024} {512*1024} -o merge_sort.csv \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"merge_sort.csv\")\n",
    "df[\"i\"] = df.index\n",
    "df[\"threshold\"] = df[\"arg1\"]\n",
    "df[\"approx threads\"] = df[\"size\"]/df['threshold']\n",
    "df[\"speedup\"] = df.iloc[0][\"ET\"]/df[\"ET\"]\n",
    "display_mono(df[[\"size\", \"threshold\", \"approx threads\", \"ET\", \"speedup\"]])\n",
    "plotPE(df=df, lines=True, what=[(\"approx threads\", \"speedup\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "The story is pretty similar:  From 2 to 4 threads gives 1.7x, while going from 4 to 8 only gives us 1.24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# OpenMP\n",
    "\n",
    "Using locks and  threads to parallelize code is notoriously tricky.  I chose the histogram and merge sort examples intentionally because they are pretty simple.  Fortunately, the compiler can help us a great deal for certain kinds of code.\n",
    "\n",
    "Compilers have had good success automatically parallelizing code for well-behaved loops.  \"Well-behaved\" means loops with loop bounds that don't change and that increment their index variables by fixed amounts.  Many programs fall into this category, including matrix multiplication and our histogram code.  Merge sort, by contrast, does not.\n",
    "\n",
    "OpenMP (Open Multi-Processing) is a widely-used and widely-supported set of extensions for C/C++ (and Fortran, if you're into that) that let the programmer guide the compiler to parallelize loops.\n",
    "\n",
    "The extensions take the form of `#pragma`s.  OpenMP has many of them, but we will use three:\n",
    "\n",
    "* `#pragma omp parallel for`\n",
    "* `#pragma omp critical`\n",
    "* `#pragma omp simd`\n",
    "\n",
    "`omp parallel for` tells the compiler to parallelize the following loop.\n",
    "\n",
    "`omp critical` tells the compiler that the next block of code should be treated as a critical section.\n",
    "\n",
    "`omp simd` tell the compiler to try to vectorize the loop.\n",
    "\n",
    "## Thread Parallelism in OpenMP\n",
    "\n",
    "Let's take a look at threads first.  Here's the multi-threaded OpenMP version of our histogram code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_OPENMP\", \"//START_OPENMP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_size = 10000000\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_openmp_histogram --size {hist_size} --threads 1 2 3 4 -o histogram_openmp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_openmp.csv\"], columns=hist_columns+[\"Cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"threads\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_dcache_misses\"]*hist_data[\"threads\"]\n",
    "display(hist_data)\n",
    "plotPE(df=hist_data, lines=True, what=[(\"threads\", \"speedup\"), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "That's awful!  But not unexpected.  One lock means no concurrency and a shared histogram array means lots of sharing.  We should have known better!\n",
    "\n",
    "Here's a better OpenMP version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"histogram.cpp\", show=(\"//START_OPENMP_PRIVATE\", \"//START_OPENMP_PRIVATE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Note that  we did the blocking ourselves this time.  OpenMP breaks the execution of the outer loop into tiles, but the local set of histogram counters, `my_histogram`, is local _to the loop iteration_ it is in.  If we didn't tile the outer loop ourselves, the lock in the `omp critical` would get taken/released on _every iteration_.  By tiling the loop ourselves, we collect a bunch of updates in `my_histogram` and then apply them all at once.   In this case, the tiling doesn't provide a locality benefit (because `data` is never reused), but it does reduce locking overhead!  \n",
    "\n",
    "In particular, by setting `arg1` to 1000, we should reduce locking overhead by about 1000 times.\n",
    "\n",
    "Let's see how this version does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cse142 job run \"rm -f build/histogram.so; make fiddle.exe; make C_OPTS='-O3 -fopenmp-simd' build/histogram.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/histogram.so -M 3300 --detail -f run_openmp_private_histogram --size {hist_size} --threads 1 2 3 4 --arg1 1000 -o histogram_openmp_private.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = render_csv([\"histogram_unthreaded.csv\", \"histogram_openmp_private.csv\"], columns=hist_columns+[\"Cycles\"])\n",
    "#hist_data[\"ET\"] = hist_data[\"ET\"]/hist_data[\"thread\"].apply(lambda x: 1 if x == 0 else x)\n",
    "hist_data[\"speedup\"] = hist_data.iloc[0][\"ET\"]/hist_data[\"ET\"]\n",
    "hist_data[\"Total IC\"] = hist_data[\"IC\"]*hist_data[\"threads\"]\n",
    "hist_data[\"Total cache misses\"] = hist_data[\"L1_dcache_misses\"]*hist_data[\"threads\"]\n",
    "display(hist_data[hist_columns])\n",
    "plotPE(df=hist_data, lines=True, what=[(\"threads\", \"speedup\"), ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Pretty good:  almost 4x speedup with 4 threads (there's some variation from run to run.).  And the code is much simpler with OpenMP!  Sounds good to me.  \n",
    "\n",
    "Especially, if I had to do PA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## SIMD Parallelism in OpenMP\n",
    "\n",
    "OpenMP also support SIMD parallelism, which uses vector instructions to improve performance.  We discussed SIMD during the 142L lecture last week.  Review the podcast, if you missed it.\n",
    "\n",
    "Applying SIMD with OpenMP is pretty easy:  You just put `#pragma omp simd` before the loop body.  The loop needs to very regular (e.g., constant stride and no branches), and there's no guarantee that the compiler will be able to vectorize it.  Indeed, `omp simd` doesn't speedup our histogram code, so we'll look at a simpler example.\n",
    "\n",
    "Here's an example of using SIMD parallelism for dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t = fiddle(\"dp.cpp\", function=\"dp\", analyze=False, opt=\"-O3\",\n",
    "code=r\"\"\"\n",
    "#include\"function_map.hpp\"\n",
    "#include<cstdint>\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t vsum(uint64_t *a, uint64_t* b, uint64_t * c, uint64_t len)\n",
    "{\n",
    "    uint64_t s = 0;\n",
    "    for(unsigned int i = 0; i < len; i++) {\n",
    "        c[i]=a[i]+b[i];\n",
    "    }\n",
    "    return s;\n",
    "}\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t vsum_simd(uint64_t *a, uint64_t* b, uint64_t * c, uint64_t len)\n",
    "{\n",
    "    uint64_t s = 0;\n",
    "#pragma omp simd\n",
    "    for(unsigned int i = 0; i < len; i++) {\n",
    "        c[i]=a[i]+b[i];\n",
    "    }\n",
    "    return s;\n",
    "}\n",
    "\n",
    "\n",
    "extern \"C\"\n",
    "uint64_t* dp(uint64_t threads, uint64_t * list, uint64_t size, uint64_t arg1, uint64_t arg2, uint64_t arg3) {\n",
    "    if(arg1 == 0) {\n",
    "        list[0] = vsum(list, &list[size/3], &list[size*2/3], size/3);\n",
    "    } else if(arg1 == 1){\n",
    "        list[0] = vsum_simd(list, &list[size/3], &list[size*2/3], size/3);\n",
    "    }\n",
    "\treturn list;\n",
    "}\n",
    "FUNCTION(one_array_2arg, dp);\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "! cse142 job run \"rm -f build/dp.so; make fiddle.exe; make MICROBENCH_OPTIMIZE='-O3 -fopenmp-simd -fopenmp' build/dp.so\"\n",
    "! cse142 job run \"./fiddle.exe -lib ./build/dp.so -M 3300 --detail -f dp --size 10000000 --threads 1 --arg1 0 1 -o dp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = render_csv(\"dp.csv\")\n",
    "df[\"opt\"] = df[\"arg1\"].apply(lambda x: [\"no SIMD\", \"SIMD\"][x])\n",
    "\n",
    "display_mono(df[[\"opt\", \"IC\", \"CPI\", \"CT\", \"ET\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "1.06x speedup for one line of code is pretty good!\n",
    "\n",
    "Note IC dropped significantly!\n",
    "\n",
    "Here's the assembly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! g++ -fopenmp -fopenmp-simd -S -O3 build/dp.cpp\n",
    "\n",
    "compare([do_render_code(\"dp.s\", show=[\"vsum\",\"LFE2984\"]),do_render_code(\"dp.s\", show=[\"vsum_simd\",\"LFE2985\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Now it's using `xmm` registers, with are 256 bits.\n",
    "\n",
    "All-in-all, SIMD is a mixed bag on our machine but it doesn't seem to hurt much and can improve performance a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "tags": []
   },
   "source": [
    "<div class=\"solution\"> \n",
    "\n",
    "All the `addq` instructions in the body read and write `rbx`, so each is dependent on the one prior to it, and there is a cross-loop dependence on `rbx` as well.\n",
    "    \n",
    "Further, the `cmpq` is dependent on the last `addq` as well, and the `jnb` is dependent on the `cmpq`. However, they don't add to the critical path for two reasons:\n",
    "    \n",
    "1. Nothing is dependent on the `jnb`\n",
    "2. The `jnb` is a very predictable branch, so it and the `cmpq` will not affect execution very much.  Likewise, we can ignore the `jmp` as well.\n",
    "    \n",
    "So, the critical for $n$ iteration is $6n$.\n",
    "\n",
    "The nominal latency for an `addq` is 1 cycle, so the latency of $n$ iterations is about $6n$ cycles.    There are 9 instructions per iteration, so `CPI` should be 6/9 = 0.67.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Programming Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Your programming lab in this lab is an extension of your work on the lab 2.  This time, you'll be parallelizing your implementation of matrix exponentiation.  You can use C++ threads, pthreads, or OpenMP.  The lab assume you'll use OpenMP.  If you use one of the others, the course staff will be less able to help you out.\n",
    "\n",
    "You are free to use your (and only your) solution to the previous lab as a starting point for this lab.\n",
    "\n",
    "Most of this section is a verbatim copy of the PA for lab 2.  The sections with new content have \"(New for lab 4)\" at the end of their names.\n",
    "\n",
    "Obviously, the performance targets have been increased.  Roughly speaking, the targets have increased by about 5x, to 40x!!!  There are six cores on our machine, and from what we have learned, it's reasonable to expect that speedup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 3,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"question correctness points-3\">\n",
    "\n",
    "### Question 11 (Correctness)     \n",
    "\n",
    "    \n",
    "<div class=\"question-text\">\n",
    "\n",
    "If you must achieve a total speedup of 4x using 4 processors, what fraction of the program must you parallelize (assuming perfect 6-thread parallelism on those parts)? \n",
    "\n",
    "</div>\n",
    "</div>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cse142.is_response": true,
    "cse142.points": 3,
    "cse142.question_type": "completeness",
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<div class= \"answer\">\n",
    "Answer:\n",
    "\n",
    "\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Performance Variability (New for Lab 4)\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Multithreaded Performance is Variable**:  Multithreading introduces variability in performance.  You will only get credit for performance numbers recorded on gradescope, you may need to run your gradescope job more than once to get meaningful measurements.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Running code on multiple processors introduce performance variability -- some times up to 20% or more.  In the real world, you'd either tolerate this or spend a lot of time trying to fix it.  Neither of those works well in a lab, because we have limited time and you need a grade that reflects the quality of your work rather than whether a run was lucky or unlucky.\n",
    "\n",
    "To address this problem, the lab includes does two things:\n",
    "1.  It runs your code 4 times and takes the average.  This is implemented in the `Makefile`.\n",
    "2.  It includes a \"canary\" program with known performance.  \n",
    "\n",
    "The canary runs before your code and if the performance of the canary is too low, the autograder will reject the run.  This means that it is much more likely that your gradescope submissions will fail and you may need to submit several times to get a good measurement. \n",
    "\n",
    "With the canary filtering out slow runs, performance variation is less this 5%.  So there is some marginal value in submitting multiple times in the hopes of getting a slightly good score.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Budget time for multiple submissions**:  Having to resubmit to gradscope due to canary failure is totally predictable, and you have been warned.  Plan to submit (and resubmit as necessary) well-ahead of the deadline. \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "## Reference Code (New for Lab 4)\n",
    "\n",
    "The reference implementation is in `matexp_reference.hpp`.  It's basically the same as for Lab 2, but it demonstrates `wall_time()` (see below) and it makes the tensor copy explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"matexp_reference.hpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Read through the code and comments to make sure you understand what the code is doing. \n",
    "\n",
    "## Detailed Requirements\n",
    "\n",
    "The requirements for the lab are pretty simple:\n",
    "\n",
    "1. $M$ will be square and it's width/height will be less than 2048.\n",
    "2. $p$ will be less than or equal to 1024.\n",
    "3. $p$ will be greater than or equal to 0.\n",
    "4. Like `matexp_reference`, your function need to be a template function, but you can assume that `T` is always `uint64_t`.\n",
    "5. Values in $M$ can be any `uint64_t` value.\n",
    "6. Your output must match the output of the code in `matexp_reference.hpp`.\n",
    "7. Your implementation should go in `matexp_solution.hpp`.  The starter version is just a copy of `matexp_reference.hpp`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Running the Code\n",
    "\n",
    "The driver code for the lab is in `matexp_main.cpp` and `matexp.cpp`.  `matexp_main.cpp` is mostly command line processing (take a look if you want).  `matexp.cpp` is what actually calls your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"matexp.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "It defines four functions:\n",
    "\n",
    "* `matexp_reference_c` Calls the starter code with `size`x`size` matrix and `power`.\n",
    "* `matexp_solution_c` Calls your code with `size`x`size` matrix and `power`.\n",
    "* `bench_reference` Runs benchmarks we will use for grading for the starter code.\n",
    "* `bench_solution` Runs benchmarks we will use for grading for your code.\n",
    "\n",
    "It runs everything 8 times to get a more reliable measurement.\n",
    "\n",
    "To invoke these, you can build and run `matexp.exe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cse142 job run 'rm matexp.exe; rm -f build/matexp*; make matexp.exe'\n",
    "!cse142 job run './matexp.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "`matexp.exe` takes several command line parameters:\n",
    "\n",
    "The notable ones are:\n",
    "\n",
    "1. `-s` -- set the size of the matrix to multiply.\n",
    "2. `-p` -- set the power to raise it to.\n",
    "3. `-f` what functions to run.\n",
    "4. `-d` sets the random seed.\n",
    "5. `-o` sets where statistics should go.\n",
    "6. `-i` sets the number of iterations.\n",
    "7. `-v` compares the result with the reference solution.\n",
    "\n",
    "The first five of these can take multiple values and `matexp.exe` will run all combinations and they will end up in `stats.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cse142 job run \"./matexp.exe -f matexp_reference_c matexp_solution_c bench_reference bench_solution --threads 1 2 4 -size 10 20 -power 1 2 4 -o stats.csv\"\n",
    "render_csv(\"stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## OpenMP (New For Lab 4)\n",
    "\n",
    "OpenMP is automatically turned for your code in this lab, so the `#pragma` command we used for the histogram should work fine.\n",
    "\n",
    "If you want to compile at the command line, you'll need to either do \n",
    "\n",
    "```\n",
    "export OPENMP=yes\n",
    "```\n",
    "\n",
    "once each time you log in or invoke `make` like so:\n",
    "\n",
    "```\n",
    "make matexp.exe OPENMP=yes\n",
    "```\n",
    "each time you build.\n",
    "\n",
    "### Key Commands\n",
    "\n",
    "The three `#pragma`s you'll need for this lab are \n",
    "\n",
    "1. `#pragma omp parallel for` for parallelizing loops.\n",
    "2. `#pragma omp critical` for parallelizing loops.\n",
    "3. `#pragma omp simd` for vectorizing loops\n",
    "\n",
    "These are the only three used in the solution used to set the performance targets.\n",
    "\n",
    "These three blog posts provide a good introduction to these commands:\n",
    "\n",
    "* http://jakascorner.com/blog/2016/04/omp-introduction.html  \n",
    "* http://jakascorner.com/blog/2016/05/omp-for.html\n",
    "* http://jakascorner.com/blog/2016/07/omp-critical.html\n",
    "\n",
    "They are required reading.\n",
    "\n",
    "These articles provide some more advanced topics that might be useful:\n",
    "\n",
    "* http://jakascorner.com/blog/2016/06/omp-for-scheduling.html\n",
    "* http://jakascorner.com/blog/2016/06/omp-data-sharing-attributes.html\n",
    "* http://jakascorner.com/blog/2016/07/omp-default-none-and-const.html\n",
    "\n",
    "There's an enormous amount of bad information online about OpenMP.\n",
    "\n",
    "### Looking at OpenMP Assembly\n",
    "\n",
    "If you look at the assembly for OpenMP programs, you'll find that your loop body has been replaced with a function call.  OpenMP does this so it can tell it's worker threads to call the function to perform one iteration of your loop.\n",
    "\n",
    "### Caveats for our Tools\n",
    "\n",
    "First, `gprof` doesn't work on with multi-threaded programs.  You can use it for single-threaded runs, though.\n",
    "\n",
    "Second, Moneta's cache model is not multithread-aware, so the cache hit/miss numbers for multithreaded programs are not meaningful.\n",
    "\n",
    "Moneta may also show more threads that you might be expecting.  OpenMP threads seem to be Thread 0 and 13 and above.  If you see some threads that don't seem to be doing anything, that's not surprising or concerning.\n",
    "\n",
    "Finally, our performance counting code only collects data for one thread.  For OpenMP code this is ok:  all the threads do basically the same thing.  But you'll notice, for instance, that if a loop runs in 4 threads, the measure instruction count will go down by ~1/4 (assuming multi-threading didn't add a lot of overhead).\n",
    " \n",
    " \n",
    "### Controlling the Number of Threads\n",
    "\n",
    "By default the autograder will run your code with 12 threads.  If you want to use a different number in your final run, you can call \n",
    "\n",
    "```\t\t\t\n",
    "omp_set_num_threads(thread_count);\n",
    "```\n",
    "\n",
    "in your function. You should call it before the first OpenMP `#pragma`.\n",
    "\n",
    "I can control thread count during development with `--thread`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Things To Try\n",
    "\n",
    "There are two main challenges I see in this programming lab:\n",
    "\n",
    "1. Make matrix multiplication fast, primarily by improving it's memory behavior.\n",
    "2. Applying matrix multiplication efficiently to compute $M^p$.\n",
    "\n",
    "The benchmarks are structured to evaluate your solution's success on both of these challenges.\n",
    "\n",
    "### Tiling Matrix Multiplication\n",
    "\n",
    "The obvious approach to improving cache performance is tiling and loop interchanging. There are two ways to approach this task and you should try to apply both at once:\n",
    "\n",
    "1.  You should think about the data access pattern of matrix multiply in terms of temporal and spatial locality.  \n",
    "    1.  How can you maximize spatial locality?\n",
    "    2.  Don't forget to consider all three matrices.\n",
    "    3.  How large can the tile size be while still fitting in the cache?\n",
    "2.  You should try different tiling schemes:\n",
    "    1.  Different ways to split and renest the three loops.\n",
    "    2.  Different tile sizes\n",
    "3.  Debugging tiling\n",
    "    1.  Debugging tiling can be tricky.\n",
    "    2.  Start with small matrices and small tile sizes.\n",
    "    3.  Try multiple small tile sizes (pass `--p*` to the regressions)\n",
    "3.  Don't forget about loop overhead.\n",
    "4.  Tiling could be effective if you do it right.\n",
    "\n",
    "There's a nice [Wikipedia page](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm) about matrix multiplication.  It covers the theory behind implementing it effectively.  The content is good, but don't assume that theory and practice will match.\n",
    "\n",
    "\n",
    "### Raising to a Power\n",
    "\n",
    "Computing $M^p$ can done more efficiently than multiplying $M$ by itself $p$ times (which is what the reference code does).  By way of a hint, remember that:\n",
    "\n",
    "$$M^{p+q} = M^pM^q$$\n",
    "\n",
    "As you work on this part of the problem, I suggest practicing with integers first.  I found it useful to code my solution with integers and test it and then rewrite it for matrices.\n",
    "\n",
    "It's possible to compute $M^p$ in $O(\\log p)$ multiplications.\n",
    "\n",
    "### Non-Deterministic Tests (New for Lab 4)\n",
    "\n",
    "With threading, comes non-deterministic bugs.  This means that the tests may fail only occasionally for your code.  If this seems to be happening, a good strategy is to just run them repeatedly and confirm that it's the case.\n",
    "\n",
    "It's not a bug in the benchmark, you have a thread synchronization error.\n",
    "\n",
    "\n",
    "### General Tips (New for Lab 4)\n",
    "\n",
    "In the examples we saw that loop tiling and OpenMP pragmas can work well together.  This carries through to how you should figure out what to parallelize.  It's worth your time to try parallelizing different loops and changing how your loops are nested to accommodate that.\n",
    "\n",
    "A few tips:\n",
    "\n",
    "1.  Use `wall_time()` (below) guide your optimizations.\n",
    "1.  At this points you have many tools available to you -- `omp parallel for`, `omp simd`, compiler optimizations, tiling.  The number of combinations is enormous.   I suggest applying them in this order (from largest impact-per-effort to smallest):\n",
    "    1.  Get last lab into good shape (see notes above and slides from class)\n",
    "    2.  `omp parallel for`\n",
    "    3.  `omp simd`\n",
    "    4.  Fiddling with other compiler options/per-function compiler options.\n",
    "    5.  Crazy stuff like intrinsics for better SIMD performance.\n",
    "1.  While tiling only applied to loops with reuse (because temporal locality requires reuse), `parallel for` can apply to loops without reuse.  Same for `omp simd`.\n",
    "2.  `omp parallel for` implicitly does tiling, since it divides the iterations of the parallel loop across several cores.\n",
    "1.  Nesting parallel for loops with OpenMP is not usually a good idea (although it should work).  Start by picking one loop to parallelize.\n",
    "2.  You want to parallelize an outer loop, so that the threads are working on large pieces of computation and need to synchronize less.\n",
    "3.  Pay close attention to whether iterations of your parallel loop are writing to the same locations.  If so, you'll need a `omp critical` to ensure correct updates.\n",
    "\n",
    "This last point can be tricky.  If I have this code:\n",
    "\n",
    "```\n",
    "#pragma omp parallel for\n",
    "for(int i = 0; i < 10; i++) {\n",
    "    for(int j = 0; j < 10; j++) {\n",
    "        for(int k = 0; k < 10; k++) {\n",
    "            X(i,j) += Y(k,j);\n",
    "```\n",
    "\n",
    "The only store is the lab to `X(i, j)`.  Since `i` is the index of the parallel loop, I know that no other thread will be updating `X(i,j)`, since no other thread will have the same value of `i`.\n",
    "\n",
    "However, in this code:\n",
    "\n",
    "```\n",
    "#pragma omp parallel for\n",
    "for(int i = 0; i < 10; i++) {\n",
    "    for(int j = 0; j < 10; j++) {\n",
    "        for(int k = 0; k < 10; k++) {\n",
    "            X(k,j) += Y(i,j);\n",
    "```\n",
    "\n",
    "I don't have the same guarantee.  Since `i` does is not used to select an element in `X`, every other thread will write to that location as well.  In that case, I could do\n",
    "\n",
    "```\n",
    "#pragma omp parallel for\n",
    "for(int i = 0; i < 10; i++) {\n",
    "    for(int j = 0; j < 10; j++) {\n",
    "        for(int k = 0; k < 10; k++) {\n",
    "#pragma omp critical\n",
    "            X(k,j) += Y(i,j);\n",
    "```\n",
    "Which will probably be really slow, or I could create a private tensor, do my updates there, and then merge them into `X`.\n",
    "\n",
    "5.  Pay close attention to write sharing when you are deciding how to parallelize.  Can the iterations of your parallel loop iterations write to the same place?\n",
    "4.  `omp simd` only works on inner loops.\n",
    "5.  `omp parallel for` works best on outer loops.\n",
    "7.  `gprof` doesn't work for multithreading, so use `wall_time()` to measure how long things take.\n",
    "\n",
    "Regarding #7:  If you've parallelize part of your program, and speedup is good but not great, use `wall_time()` to identify serial parts of your code that are slow.   The problem may be that your $x$ in Amdahl's Law is too small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Useful C++ ( Partly New For Lab 4) \n",
    "\n",
    "#### `wall_time()` (New for Lab 4)\n",
    "\n",
    "We've provide `wall_time()` which returns the current time as a `double`.  It has microsecond accuracy.  You can call it before and after part of your program, subtract the resulting values, and get a pretty good measure of execution time.  There's an example in `matexp_reference.hpp`.\n",
    "\n",
    "#### Controlling Compiler Optimizations\n",
    "\n",
    "First, you can prevent inlining of a particular function by declaring it like so:\n",
    "\n",
    "```\n",
    "void __attribute__((noinline)) matexp_solution(...)\n",
    "```\n",
    "\n",
    "This can make it easier to debug, because you can set a breakpoint on the function and it'll work like you expect.\n",
    "\n",
    "Second, you can turn on arbitrary optimizations for particular functions like so:\n",
    "\n",
    "```\n",
    "#pragma GCC push_options\n",
    "#pragma GCC optimize (\"unroll-loops\")\n",
    "\n",
    "void your_function() {\n",
    "}\n",
    "\n",
    "#pragma GCC pop_options\n",
    "```\n",
    "\n",
    "\n",
    "#### Assertions\n",
    "\n",
    "The `assert()` macro is useful tool for debugging and to avoid silly errors.\n",
    "\n",
    "If you say\n",
    "\n",
    "```\n",
    "assert(a > b);\n",
    "```\n",
    "\n",
    "And the expression is not true at run time, the assert with \"fail\" your program will crash with a somewhat useful error message.\n",
    "\n",
    "This is a useful way to document and enforce assumptions you make in your code.  For instance, I used an assert in `convolution_tiled_split()` to ensure that the tile size was > 8.\n",
    "\n",
    "You can get access to  `assert()` with \n",
    "\n",
    "```\n",
    "#include<cassert>\n",
    "```\n",
    "\n",
    "The overhead of asserts is low, but not zero.  I would not put any in one of your performance-critical loops.\n",
    "\n",
    "If you want to include asserts in performance-critical areas, you can add `-DNDEBUG` to the optimizations in `config.make`.  It'll disable all the `assert()`s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Do Your Work Here\n",
    "\n",
    "Below are the key commands you'll need to make progress on the lab.\n",
    "\n",
    "### Setting Optimization Flags\n",
    "\n",
    "As in your last lab, you can set optimization flags in `config.make`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"config.make\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Compiling and Running\n",
    "\n",
    "You can compile and the benchmarks locally using this command.  This is only useful for debugging.  Performance running locally is not very meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!make matexp.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the benchmark in the cloud and compare your performance with the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cse142 job run \"make clean\"\n",
    "!cse142 job run \"make matexp.exe\"\n",
    "!cse142 job run \"./matexp.exe -MHz 3300 --detail -i 10 --threads 1 4 8 -f bench_solution bench_reference\"\n",
    "df = render_csv(\"stats.csv\")\n",
    "display_mono(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Final Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Running the cell does just what the Gradescope autograder does.  And the cell below shows the name and target speedups for each benchmark.  This takes 1-2 minutes to run.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Only Gradescope Counts** The scores produced here **do not** count.  Only gradescope counts.  The results here should match what Gradescope does, but I would test your solution on Gradescope well-ahead of the deadline to ensure your code is working like you expect.\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**The autograder doesn't pass additional parameters**. You'll need to set up the optimal configurations your code in the best way possible.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f build/matexp*\n",
    "!cse142 job run \"make autograde\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "And run the autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p autograde; cp bench.csv autograde; cp correctness.csv autograde\n",
    "!./autograde.py --submission autograde --results autograde.json\n",
    "from autograde import compute_all_scores\n",
    "df = compute_all_scores(dir=\"autograde\")\n",
    "display_mono(df)\n",
    "print(f\"total points (performance): {round(sum(df['capped_score']), 2)}\")\n",
    "display_mono(render_csv(\"correctness.csv\"))\n",
    "corrects = compute_correctness(dir=\"autograde\")\n",
    "print(f\"correctness points: {corrects/6*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "The \"capped_score\" column contains the number of points you'll receive.\n",
    "\n",
    "And see the autograder's output like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_code(\"autograde.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Most of it is internal stuff that gradscope needs, but the key parts are the `score`, `max_score`, and `output` fields.\n",
    "\n",
    "All that's left is commit your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git commit -am \"Solution to the lab.\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "If `git push` asks for your username, you'll need to push from the command line.\n",
    "\n",
    "If `git commit` tells you have uncommitted files, that's not a problem. \n",
    "\n",
    "If `git commit` tell you something like:\n",
    "\n",
    "```\n",
    "*** Please tell me who you are.\n",
    "\n",
    "Run\n",
    "\n",
    "git config --global user.email \"you@example.com\"\n",
    "git config --global user.name \"Your Name\"\n",
    "\n",
    "to set your account's default identity.\n",
    "Omit --global to set the identity only in this repository.\n",
    "\n",
    "fatal: unable to auto-detect email address (got 'prcheng@dsmlp-jupyter-prcheng.(none)')\n",
    "Warning: Permanently added the RSA host key for IP address '140.82.112.3' to the list of known hosts.\n",
    "Everything up-to-date\n",
    "```\n",
    "\n",
    "Then you can do (but fill in your @ucr.edu email and your name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cse142.is_response": true,
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "source": [
    "This lab completes our tour of exploiting modern processor (multi-threaded processors) features.  It explored what's required to exploit thread-level parallelism. It shows examples achieving better TLPs is really difficult -- coherency, consistency, locks ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Turning In the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "For each lab, there are two different labs on gradescope:\n",
    "\n",
    "1.  The lab notebook.\n",
    "2.  The programming lab.\n",
    "3.  A post-lab survey which is embedded below.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**NOTE:** Filling out the form above _before_ the deadline is the _only_ mechanism available to receive credit without turning in the lab on time.\n",
    "    \n",
    "</div>\n",
    "\n",
    "If it's more than a day before the deadline, you can reach out via Piazza and hopefully we can get it sorted out.\n",
    "## The Note Book\n",
    "\n",
    "You need to turn in your lab notebook and your programming lab separately. \n",
    "\n",
    "After you complete the lab, you will turn it in by creating a version of the notebook that only contains your answers and then printing that to a pdf.\n",
    "\n",
    "**Step 1:**  Save your workbook!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!for i in 1 2 3 4 5; do echo Save your notebook!; sleep 1; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**Step 2:**  Run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cse142 turnin Lab.ipynb\n",
    "!ls -lh Lab.turnin.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "The date in the above file listing should show that you just created `Lab.turnin.ipynb`\n",
    "\n",
    "**Step 3:**  Click on this link to open it: [./Lab.turnin.ipynb](./Lab.turnin.ipynb)\n",
    "\n",
    "\n",
    "**Step 4:**  Select \"Save and export notebook as\" from _your jupyterhub's_ \"file\" menu and select \"PDF\".\n",
    "\n",
    "**Step 5:**  Make sure all your answers are visible and not cut off the side of the page.\n",
    "\n",
    "**Step 6:**  Turn in that PDF via gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## The Programming Lab\n",
    "\n",
    "You'll turn in your programming lab by providing gradescope with your github repo.   It'll run the autograder and return the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "\n",
    "## Post-lab Survey\n",
    "\n",
    "Please fill out this survey when you've finished the lab.  You can only submit once.  Be sure to press \"submit\", your answers won't be saved in the notebook.\n",
    "\n",
    "https://forms.gle/xcBXm79Uxt673iC57\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
